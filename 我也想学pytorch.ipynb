{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhw7ZvQc9TJm",
        "outputId": "6ebed55d-f86a-4bc0-8f42-fc4f9da10521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #三通道输入，十输出，2*2池化，2卷积3fc\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def average_weights(w):\n",
        "    \"\"\"\n",
        "    Returns the average of the weights.\n",
        "    \"\"\"\n",
        "    w_avg = copy.deepcopy(w[0])\n",
        "    for key in w_avg.keys():\n",
        "        for i in range(1, len(w)):\n",
        "            w_avg[key] += w[i][key]\n",
        "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
        "    return w_avg\n",
        "\n",
        "#CIFAR-10数据集包含10个类别，每个类别有6000张32x32的彩色图像。\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 100 #单次批次大小\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "ajsIMtiA9DDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febb1680-d574-451b-890a-9698be5deb28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPIMAUcQ1zMf"
      },
      "outputs": [],
      "source": [
        "#以下用于直接学，就嗯学\n",
        "#定义神经网络\n",
        "device='cuda:0'\n",
        "net = Net().to(device)  # Move the model to GPU\n",
        "net.zero_grad()  # zeroes the gradient buffers of all parameters\n",
        "print(net)\n",
        "\n",
        "#定义损失函数\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "#定义更新函数（SGD）\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "#测试输入集{input,target}\n",
        "#input = torch.randn(1, 1, 32, 32).to(device)  # Move input tensor to GPU\n",
        "#target = torch.randn(10).to(device)  # a dummy target, for example\n",
        "#target = target.view(1, -1)  # make it the same shape as output\n",
        "\n",
        "#前向传播并计算损失\n",
        "#output = net(input)\n",
        "#print(output)\n",
        "#loss = criterion(output, target)\n",
        "#print(loss)\n",
        "\n",
        "#清除优化器累计梯度更新\n",
        "#optimizer.zero_grad()   # zero the gradient buffers\n",
        "\n",
        "#更新模型\n",
        "#loss.backward()   #后向传播计算梯度\n",
        "#optimizer.step()    # 更新模型参数\n",
        "\n",
        "#载入训练集\n",
        "#特征，标签 特征为图片集，标签为各图片集对应class在classes中的位置\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "#训练神经网络\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "#保存训练好的神经网络模型\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#以下用于测试联邦学习\n",
        "class main_node():\n",
        "  def __init__(self,mydevice='cpu',myglepoch=3,mynodenum=3,mysubepoch=3):\n",
        "    self.device=mydevice\n",
        "    self.net=Net()\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    if self.device!='cpu':\n",
        "      self.net=self.net.to(self.device)\n",
        "      self.criterion = self.criterion.to(self.device)\n",
        "    self.optimizer = optim.SGD(self.net.parameters(), lr=0.001, momentum=0.9)\n",
        "    self.global_epoch=myglepoch\n",
        "    self.node_num=mynodenum\n",
        "    self.sub_epoch=mysubepoch\n",
        "  def federated_train(self):\n",
        "    train_loss, train_accuracy = [], []\n",
        "    for global_epoch_i in range(self.global_epoch):  #全局轮\n",
        "      print('Current Global Epoch: '+str(global_epoch_i))\n",
        "      global_epoch_weights, global_epoch_local_losses = [], []\n",
        "      for sub_id in range(self.node_num): #子节点\n",
        "        print('Current Sub_id: '+str(sub_id))\n",
        "        sub=sub_node(copy.deepcopy(self.net),mydevice='cuda:0')\n",
        "        subweights,subloss=sub.train_net(trainloader,self.sub_epoch)\n",
        "        global_epoch_weights.append(copy.deepcopy(subweights))\n",
        "        global_epoch_local_losses.append(copy.deepcopy(subloss))\n",
        "      #全局轮次更新\n",
        "      self.net.load_state_dict(average_weights(global_epoch_weights))\n",
        "      loss_avg = sum(global_epoch_local_losses) / len(global_epoch_local_losses)\n",
        "      train_loss.append(loss_avg)\n",
        "class sub_node(main_node):\n",
        "  def __init__(self,mynet,mydevice='cpu'):\n",
        "    self.device=mydevice\n",
        "    self.net=mynet\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    if self.device!='cpu':\n",
        "      self.net=self.net.to(self.device)\n",
        "      self.criterion = self.criterion.to(self.device)\n",
        "    self.optimizer = optim.SGD(self.net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  def train_net(self,trainset,subepoch):\n",
        "    epoch_loss = []\n",
        "    for epoch_i in range(subepoch):  #本地轮\n",
        "      batch_loss = []\n",
        "      print('Current Sub Epoch: '+str(epoch_i))\n",
        "      for batch_i, (images, labels) in enumerate(trainset):   #对于各批次\n",
        "        if self.device!='cpu':\n",
        "          images, labels = images.to(self.device), labels.to(self.device)\n",
        "        self.optimizer.zero_grad() #FL案例里清零的是net，一想一个不吱声\n",
        "        outputs=self.net(images)\n",
        "        loss = self.criterion(outputs, labels)  #计算更新\n",
        "        loss.backward() #后向传播计算梯度\n",
        "        self.optimizer.step() #根据梯度修正参数\n",
        "        batch_loss.append(loss.item())\n",
        "      epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "    return self.net.state_dict(), sum(epoch_loss) / len(epoch_loss) #回传参数和损失\n",
        "\n",
        "rootNode=main_node(mydevice='cuda:0',myglepoch=5,mysubepoch=5)\n",
        "rootNode.federated_train()\n",
        "#保存训练好的神经网络模型\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(rootNode.net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "idxjwBPRzhPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#以下代码用于异步多线程执行联邦学习(实测没有加速效果，计算密集度太高)\n",
        "import threading\n",
        "\n",
        "class main_node():\n",
        "  def __init__(self,mydevice='cpu',myglepoch=3,mynodenum=3,mysubepoch=3):\n",
        "    self.device=mydevice\n",
        "    self.net=Net()\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    if self.device!='cpu':\n",
        "      self.net=self.net.to(self.device)\n",
        "      self.criterion = self.criterion.to(self.device)\n",
        "    self.optimizer = optim.SGD(self.net.parameters(), lr=0.001, momentum=0.9)\n",
        "    self.global_epoch=myglepoch\n",
        "    self.node_num=mynodenum\n",
        "    self.sub_epoch=mysubepoch\n",
        "\n",
        "  def federated_train(self):\n",
        "    train_loss, train_accuracy = [], []\n",
        "    for global_epoch_i in range(self.global_epoch):  #全局轮\n",
        "      print('Current Global Epoch: '+str(global_epoch_i))\n",
        "      global_epoch_weights, global_epoch_local_losses = [], []\n",
        "      threads = []\n",
        "      for sub_id in range(self.node_num): #子节点\n",
        "        sub=sub_node(copy.deepcopy(self.net),sub_id,mydevice='cuda:0')\n",
        "        thread = threading.Thread(target=self.train_sub_node, args=(sub, trainloader, self.sub_epoch, global_epoch_weights, global_epoch_local_losses))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "      for thread in threads:\n",
        "        thread.join()\n",
        "      #全局轮次更新\n",
        "      self.net.load_state_dict(average_weights(global_epoch_weights))\n",
        "      loss_avg = sum(global_epoch_local_losses) / len(global_epoch_local_losses)\n",
        "      train_loss.append(loss_avg)\n",
        "\n",
        "  def train_sub_node(self, sub, trainloader, subepoch, global_epoch_weights, global_epoch_local_losses):\n",
        "    subweights,subloss=sub.train_net(trainloader,subepoch)\n",
        "    global_epoch_weights.append(copy.deepcopy(subweights))\n",
        "    global_epoch_local_losses.append(copy.deepcopy(subloss))\n",
        "\n",
        "class sub_node(main_node):\n",
        "  def __init__(self,mynet,mysubid,mydevice='cpu'):\n",
        "    self.device=mydevice\n",
        "    self.sub_id=mysubid\n",
        "    self.net=mynet\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    if self.device!='cpu':\n",
        "      self.net=self.net.to(self.device)\n",
        "      self.criterion = self.criterion.to(self.device)\n",
        "    self.optimizer = optim.SGD(self.net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  def train_net(self,trainset,subepoch):\n",
        "    epoch_loss = []\n",
        "    for epoch_i in range(subepoch):  #本地轮\n",
        "      batch_loss = []\n",
        "      print('Sub: '+str(self.sub_id)+' Epoch: '+str(epoch_i))\n",
        "      for batch_i, (images, labels) in enumerate(trainset):   #对于各批次\n",
        "        if self.device!='cpu':\n",
        "          images, labels = images.to(self.device), labels.to(self.device)\n",
        "        self.optimizer.zero_grad() #FL案例里清零的是net，一想一个不吱声\n",
        "        outputs=self.net(images)\n",
        "        loss = self.criterion(outputs, labels)  #计算更新\n",
        "        loss.backward() #后向传播计算梯度\n",
        "        self.optimizer.step() #根据梯度修正参数\n",
        "        batch_loss.append(loss.item())\n",
        "      epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "    return self.net.state_dict(), sum(epoch_loss) / len(epoch_loss) #回传参数和损失\n",
        "\n",
        "rootNode=main_node(mydevice='cuda:0',myglepoch=3,mynodenum=5,mysubepoch=3)\n",
        "rootNode.federated_train()\n",
        "#保存训练好的神经网络模型\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(rootNode.net.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "12Kxl7B9GTxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#以下用于从本地载入模型进行测试\n",
        "#载入测试集\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "#加载本地模型\n",
        "device='cuda:0'\n",
        "net = Net().to(device)\n",
        "PATH = './cifar_net.pth'\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "#推理测试\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images=images.to(device)\n",
        "        labels=labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) #data中每个结果的最大值,data中每个结果的最大值索引\n",
        "        total += labels.size(0) #本批次测试数量\n",
        "        correct += (predicted == labels).sum().item() #本批次正确数量\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "4RyVGAtTIakY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}